{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709762, 21)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "tensorflow_commits = pd.read_csv('C:/Users/aveli/Downloads/tensorflow.csv')\n",
    "\n",
    "\n",
    "#tensorflow_commits = pd.read_csv('/home/kc/Projects/data_files/tensorflow.csv')\n",
    "vscode_commits=pd.read_csv('C:/Users/aveli/Downloads/vscode.csv')\n",
    "react_commits=pd.read_csv('C:/Users/aveli/Downloads/react-native.csv')\n",
    "\n",
    "total_commits=tensorflow_commits.append(vscode_commits, ignore_index=True)\n",
    "total_commits=total_commits.append(react_commits, ignore_index=True)\n",
    "                             \n",
    "total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aveli\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(709762, 28)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating various features for each modification\n",
    "# Total number of lines changed\n",
    "total_commits['total_changed'] = total_commits['lines_added'] + total_commits['lines_removed']\n",
    "\n",
    "# Fraction of lines changed per total numbe of lines in file\n",
    "# We need to account for the fact that new files added with have existing size as '0' and divide by '0' is indeterminate\n",
    "total_commits['size'].loc[total_commits['size'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed'] / total_commits['size']\n",
    "\n",
    "# Need to weigh the complexity by quantum of change. \n",
    "total_commits['rated_complexity'] = total_commits['ratio_changed'] * total_commits['complexity'] * total_commits['total_changed']\n",
    "\n",
    "# weighing the dmm params by the total changed lines\n",
    "total_commits['total_dmm_size'] = total_commits['total_changed'] * total_commits['dmm_unit_size']\n",
    "total_commits['total_dmm_unit_complexity'] = total_commits['total_changed'] * total_commits['dmm_unit_complexity']\n",
    "total_commits['total_dmm_unit_interfacing'] = total_commits['total_changed'] * total_commits['dmm_unit_interfacing']\n",
    "\n",
    "# We picked the sqrt of no_of_mod_files to reduce weightage of this feature\n",
    "total_commits['scaled_rated_complexity']=total_commits['rated_complexity'] * (total_commits['no._of_mod_files'] ** 0.5)\n",
    "\n",
    "total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709762, 8)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data. ML requires the data to be converted to numericals\n",
    "#ml_commits = total_commits[['hash','Author', 'no._of_mod_files', 'dmm_unit_size',\n",
    "#       'dmm_unit_complexity', 'dmm_unit_interfacing', 'complexity', 'functions', 'lines_added', 'lines_removed', \n",
    "#       'tokens', 'type']]\n",
    "\n",
    "ml_commits = total_commits[['hash','Author','total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "# Temporarily dropping text columns for numeric processing\n",
    "ml_commits_noText = ml_commits.drop(columns = ['Author','hash'])\n",
    "\n",
    "# Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "# Adding the Author column back to create a 'total' data frame\n",
    "ml_commits_total = ml_commits_numeric.copy()\n",
    "ml_commits_total['Author'] = ml_commits['Author']\n",
    "ml_commits_total['hash'] = ml_commits['hash']\n",
    "\n",
    "print(ml_commits_total.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to remove outliers. May not be required\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "# Calculate z_scores (and if zscore is greater than '3', then its an outlier) and collect normal subset.\n",
    "ml_commits_nout = ml_commits_total[(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_nout.to_csv('C:/Users/aveli/Downloads/totalCommits_nout.csv')\n",
    "\n",
    "# Collect outliers\n",
    "ml_commits_out = ml_commits_total[~(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_out.to_csv('C:/Users/aveli/Downloads/totalCommits_out.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying scaler to regular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Use minMax scaler since this does not distort\n",
    "# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "scaler = MinMaxScaler()\n",
    "ml_commits_nout_numeric = ml_commits_nout.drop(columns = ['Author','hash'])\n",
    "data_scaled = scaler.fit_transform(ml_commits_nout_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.80774932e-03, 7.12723923e-07, 8.05812368e-04, 6.76028487e-04,\n",
       "        9.01465246e-04, 2.24471370e-07],\n",
       "       [5.48828399e-02, 8.66249956e-04, 4.16912417e-02, 3.01486633e-02,\n",
       "        4.47182590e-02, 2.45470616e-04],\n",
       "       [2.74691436e-01, 1.74056240e-02, 1.59041418e-01, 1.35382271e-01,\n",
       "        1.69151256e-01, 9.09486254e-03],\n",
       "       [1.25222674e-02, 4.88573676e-05, 1.11096466e-02, 8.15527423e-03,\n",
       "        1.21797644e-02, 1.43515101e-05],\n",
       "       [2.03943134e-02, 8.53335747e-06, 2.65581311e-05, 2.47453989e-05,\n",
       "        4.09561424e-05, 1.11290198e-06]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix = GaussianMixture(n_components=5,random_state=42)\n",
    "s = mix.fit(data_scaled)\n",
    "cluster_frame = pd.DataFrame(data_scaled)\n",
    "gmmhash_clusters = mix.predict(cluster_frame)\n",
    "gmmcentroids = mix.means_\n",
    "gmmcentroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aveli\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\users\\aveli\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\aveli\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_changed</th>\n",
       "      <th>rated_complexity</th>\n",
       "      <th>total_dmm_unit_complexity</th>\n",
       "      <th>total_dmm_size</th>\n",
       "      <th>total_dmm_unit_interfacing</th>\n",
       "      <th>scaled_rated_complexity</th>\n",
       "      <th>Author</th>\n",
       "      <th>hash</th>\n",
       "      <th>center</th>\n",
       "      <th>fixed_cluster</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>13.177872</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.175000</td>\n",
       "      <td>7.525000</td>\n",
       "      <td>26.355745</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>573.979592</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>38.437500</td>\n",
       "      <td>40.312500</td>\n",
       "      <td>1147.959184</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>266.518875</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>24.087500</td>\n",
       "      <td>25.262500</td>\n",
       "      <td>533.037750</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>58.992683</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>117.985366</td>\n",
       "      <td>George Karpenkov</td>\n",
       "      <td>ef47bbbd57cba8fcc7ae11df8c7141d6c68ba0d0</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709757</th>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.586576</td>\n",
       "      <td>99.679797</td>\n",
       "      <td>156.212240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709758</th>\n",
       "      <td>41</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>31.297426</td>\n",
       "      <td>23.353552</td>\n",
       "      <td>36.598296</td>\n",
       "      <td>4808.025790</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709759</th>\n",
       "      <td>157</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>119.846242</td>\n",
       "      <td>89.427018</td>\n",
       "      <td>140.144696</td>\n",
       "      <td>3068.536785</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709760</th>\n",
       "      <td>574</td>\n",
       "      <td>33866.000000</td>\n",
       "      <td>438.163968</td>\n",
       "      <td>326.949734</td>\n",
       "      <td>512.376148</td>\n",
       "      <td>661904.883795</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.764767</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709761</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.740333</td>\n",
       "      <td>10.252779</td>\n",
       "      <td>16.067545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>708848 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        total_changed  rated_complexity  total_dmm_unit_complexity  \\\n",
       "0                  14         13.177872                  14.000000   \n",
       "1                  75        573.979592                  75.000000   \n",
       "2                   4          0.301887                   4.000000   \n",
       "3                  47        266.518875                  47.000000   \n",
       "4                  19         58.992683                  18.050000   \n",
       "...               ...               ...                        ...   \n",
       "709757            175          0.000000                 133.586576   \n",
       "709758             41        246.000000                  31.297426   \n",
       "709759            157        157.000000                 119.846242   \n",
       "709760            574      33866.000000                 438.163968   \n",
       "709761             18          0.000000                  13.740333   \n",
       "\n",
       "        total_dmm_size  total_dmm_unit_interfacing  scaled_rated_complexity  \\\n",
       "0             7.175000                    7.525000                26.355745   \n",
       "1            38.437500                   40.312500              1147.959184   \n",
       "2             2.050000                    2.150000                 0.603774   \n",
       "3            24.087500                   25.262500               533.037750   \n",
       "4            18.050000                   18.050000               117.985366   \n",
       "...                ...                         ...                      ...   \n",
       "709757       99.679797                  156.212240                 0.000000   \n",
       "709758       23.353552                   36.598296              4808.025790   \n",
       "709759       89.427018                  140.144696              3068.536785   \n",
       "709760      326.949734                  512.376148            661904.883795   \n",
       "709761       10.252779                   16.067545                 0.000000   \n",
       "\n",
       "                        Author                                      hash  \\\n",
       "0       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "1       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "2       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "3       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "4             George Karpenkov  ef47bbbd57cba8fcc7ae11df8c7141d6c68ba0d0   \n",
       "...                        ...                                       ...   \n",
       "709757              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709758              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709759              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709760              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709761              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "\n",
       "          center  fixed_cluster  Cluster  \n",
       "0       0.004192              0        0  \n",
       "1       0.044030              2        3  \n",
       "2       0.004192              0        0  \n",
       "3       0.044030              2        3  \n",
       "4       0.044030              2        3  \n",
       "...          ...            ...      ...  \n",
       "709757  0.172553              3        1  \n",
       "709758  0.044030              2        3  \n",
       "709759  0.172553              3        1  \n",
       "709760  0.764767              4        2  \n",
       "709761  0.044030              2        3  \n",
       "\n",
       "[708848 rows x 11 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combinedCentroids = gmmcentroids[gmmhash_clusters].sum(axis=1)\n",
    "# adding column with combined centroid values to the original dataframe \n",
    "ml_commits_nout['center'] = combinedCentroids\n",
    "#print(combinedCentroids)\n",
    "# Creating a dictionary with combined centroid values and target cluster labels\n",
    "unique_centroids = np.unique(combinedCentroids).tolist()\n",
    "cluster_labels = np.arange(5).tolist()\n",
    "cluster_dict = dict(zip(unique_centroids,cluster_labels))\n",
    "#print(g)\n",
    "ml_commits_nout['fixed_cluster'] = ml_commits_nout['center'].map(cluster_dict)\n",
    "# Converting the input data series into pan\n",
    "ml_commits_nout['Cluster'] = gmmhash_clusters\n",
    "#ml_commits_nout[ml_commits_nout['Cluster']==0]\n",
    "#ml_commits_nout['sum_value'] = ml_commits_nout['scaled_rated_complexity']+ml_commits_nout['total_dmm_unit_interfacing']+ ml_commits_nout['total_dmm_size']+ml_commits_nout['total_dmm_unit_complexity']+ml_commits_nout['rated_complexity']+ml_commits_nout['ratio_changed']+ml_commits_nout['total_changed']\n",
    "#ml_commits_nout[ml_commits_nout['Cluster']==3]  \n",
    "ml_commits_nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values of inverted scaling of centroids for sanity\n",
    "real_centroids = scaler.inverse_transform(gmmcentroids)\n",
    "\n",
    "# Write these to dataframe\n",
    "real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity'])\n",
    "\n",
    "# Add a cloumn for summing all coloumns\n",
    "real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "\n",
    "#You can write it out as csv if required.\n",
    "real_centroids_dataFrame.to_csv('C:/Users/aveli/Downloads/totalCommits_centroids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fixed_cluster\n",
       "0    472234\n",
       "1     68990\n",
       "2    110468\n",
       "3     47968\n",
       "4      9188\n",
       "Name: total_changed, dtype: int64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_commits_nout.groupby('fixed_cluster')['total_changed'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cluster\n",
       "0    472234\n",
       "1     47968\n",
       "2      9188\n",
       "3    110468\n",
       "4     68990\n",
       "Name: total_changed, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_commits_nout.groupby('Cluster')['total_changed'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ml_commits_nout_numeric_xg = ml_commits_nout.drop(columns=['Author','hash','center','Cluster'])\n",
    "\n",
    "\n",
    "# Prepare the 'X' and 'Y' for the model\n",
    "X_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg.drop(columns = ['fixed_cluster'])\n",
    "Y_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg['fixed_cluster']\n",
    "\n",
    "# Split the data for 'Training' and 'Testing' datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ml_commits_nout_numeric_xg, Y_ml_commits_nout_numeric_xg, random_state=7)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboostmodel = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboostmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.998419971559488\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgboostmodel.predict(X_test)\n",
    " \n",
    "# accuracy on X_test \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#sameaccuracy = xgboostmodel.score(X_test, y_test) \n",
    "print('accuracy', accuracy)\n",
    "#print('sameaccuracy', sameaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_changed', 'rated_complexity', 'total_dmm_unit_complexity',\n",
       "       'total_dmm_size', 'total_dmm_unit_interfacing',\n",
       "       'scaled_rated_complexity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'C:/Users/aveli/Downloads/finalized_model.sav'\n",
    "pickle.dump(xgboostmodel, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
