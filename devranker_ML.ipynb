{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(709762, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tensorflow_commits = pd.read_csv('C:/Users/aveli/Downloads/tensorflow.csv')\n",
    "vscode_commits=pd.read_csv('C:/Users/aveli/Downloads/vscode.csv')\n",
    "react_commits=pd.read_csv('C:/Users/aveli/Downloads/react-native.csv')\n",
    "\n",
    "total_commits=tensorflow_commits.append(vscode_commits, ignore_index=True)\n",
    "total_commits=total_commits.append(react_commits, ignore_index=True)\n",
    "                             \n",
    "total_commits.shape\n",
    "                             \n",
    "\n",
    "#total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use h2o4gpu if you have it installed\n",
    "\n",
    "# Add a flag with Default as False. Don't change this unless your kernel uses h2o4gpu.\n",
    "h2o4gpu_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aveli\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(709762, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating various features for each modification\n",
    "# Total number of lines changed\n",
    "total_commits['total_changed'] = total_commits['lines_added'] + total_commits['lines_removed']\n",
    "\n",
    "# Fraction of lines changed per total numbe of lines in file\n",
    "# We need to account for the fact that new files added with have existing size as '0' and divide by '0' is indeterminate\n",
    "total_commits['size'].loc[total_commits['size'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed'] / total_commits['size']\n",
    "\n",
    "# Need to weigh the complexity by quantum of change. \n",
    "total_commits['rated_complexity'] = total_commits['ratio_changed'] * total_commits['complexity'] * total_commits['total_changed']\n",
    "\n",
    "# weighing the dmm params by the total changed lines\n",
    "total_commits['total_dmm_size'] = total_commits['total_changed'] * total_commits['dmm_unit_size']\n",
    "total_commits['total_dmm_unit_complexity'] = total_commits['total_changed'] * total_commits['dmm_unit_complexity']\n",
    "total_commits['total_dmm_unit_interfacing'] = total_commits['total_changed'] * total_commits['dmm_unit_interfacing']\n",
    "\n",
    "# We picked the sqrt of no_of_mod_files to reduce weightage of this feature\n",
    "total_commits['scaled_rated_complexity']=total_commits['rated_complexity'] * (total_commits['no._of_mod_files'] ** 0.5)\n",
    "\n",
    "total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709762, 8)\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing the data. ML requires the data to be converted to numericals\n",
    "#ml_commits = total_commits[['hash','Author', 'no._of_mod_files', 'dmm_unit_size',\n",
    "#       'dmm_unit_complexity', 'dmm_unit_interfacing', 'complexity', 'functions', 'lines_added', 'lines_removed', \n",
    "#       'tokens', 'type']]\n",
    "\n",
    "ml_commits = total_commits[['hash','Author','total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "# Temporarily dropping text columns for numeric processing\n",
    "ml_commits_noText = ml_commits.drop(columns = ['Author','hash'])\n",
    "\n",
    "# Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "# Adding the Author column back to create a 'total' data frame\n",
    "ml_commits_total = ml_commits_numeric.copy()\n",
    "ml_commits_total['Author'] = ml_commits['Author']\n",
    "ml_commits_total['hash'] = ml_commits['hash']\n",
    "\n",
    "print(ml_commits_total.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to remove outliers. May not be required\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "# Calculate z_scores (and if zscore is greater than '3', then its an outlier) and collect normal subset.\n",
    "ml_commits_nout = ml_commits_total[(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_nout.to_csv('C:/Users/aveli/Downloads/totalCommits_nout.csv')\n",
    "\n",
    "# Collect outliers\n",
    "ml_commits_out = ml_commits_total[~(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_out.to_csv('C:/Users/aveli/Downloads/totalCommits_out.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying scaler to regular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "if (h2o4gpu_enabled == True):\n",
    "    from h2o4gpu.preprocessing import MinMaxScaler\n",
    "else:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "# Use minMax scaler since this does not distort\n",
    "# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "scaler = MinMaxScaler()\n",
    "ml_commits_nout_numeric = ml_commits_nout.drop(columns = ['Author','hash'])\n",
    "data_scaled = scaler.fit_transform(ml_commits_nout_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia. \\nPlease ignore this cell entirely.\\n\\n\\nimport h2o4gpu\\n\\nSSE = []\\n\\nfor cluster in range(1,20):\\n    kmeans_ss = h2o4gpu.KMeans(n_gpus=1, n_clusters = cluster, init=\\'k-means++\\', random_state = 42, backend=h2o4gpu)\\n    %time kmeans_ss.fit(data_scaled)\\n    SSE.append(kmeans_ss.inertia_)\\n    print(kmeans_ss.cluster_centers_)\\n    print(len(SSE))\\n    print(str(SSE))\\n    \\n    # converting the results into a dataframe and plotting them\\nframe = pd.DataFrame({\\'Cluster\\':range(1,20), \\'SSE\\':SSE})\\nplt.figure(figsize=(12,6))\\nplt.plot(frame[\\'Cluster\\'], frame[\\'SSE\\'], marker=\\'o\\')\\nplt.xlabel(\\'Number of clusters\\')\\nplt.ylabel(\\'Inertia\\')\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "\n",
    "''' Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia. \n",
    "Please ignore this cell entirely.\n",
    "\n",
    "\n",
    "import h2o4gpu\n",
    "\n",
    "SSE = []\n",
    "\n",
    "for cluster in range(1,20):\n",
    "    kmeans_ss = h2o4gpu.KMeans(n_gpus=1, n_clusters = cluster, init='k-means++', random_state = 42, backend=h2o4gpu)\n",
    "    %time kmeans_ss.fit(data_scaled)\n",
    "    SSE.append(kmeans_ss.inertia_)\n",
    "    print(kmeans_ss.cluster_centers_)\n",
    "    print(len(SSE))\n",
    "    print(str(SSE))\n",
    "    \n",
    "    # converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "# You don't need to run this for every iteration. Just uncomment and run whenever you need to\n",
    "\n",
    "# Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia.\n",
    "# http://docs.h2o.ai/h2o4gpu/latest-stable/h2o4gpu-py-docs/html/_modules/h2o4gpu/solvers/kmeans.html\n",
    "\n",
    "# Set this to 'True' if you want to plot graph to find elbow\n",
    "find_elbow = False\n",
    "\n",
    "if (find_elbow == True): \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    SSE = []\n",
    "\n",
    "    for cluster in range(1,20):\n",
    "        %time kmeans_ss = KMeans(n_clusters = cluster, init='k-means++', random_state = 42)\n",
    "        kmeans_ss.fit(data_scaled)\n",
    "        SSE.append(kmeans_ss.inertia_)\n",
    "\n",
    "    # converting the results into a dataframe and plotting them\n",
    "    frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aveli\\python37\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\aveli\\python37\\lib\\site-packages\\ipykernel_launcher.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "c:\\users\\aveli\\python37\\lib\\site-packages\\ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_changed</th>\n",
       "      <th>rated_complexity</th>\n",
       "      <th>total_dmm_unit_complexity</th>\n",
       "      <th>total_dmm_size</th>\n",
       "      <th>total_dmm_unit_interfacing</th>\n",
       "      <th>scaled_rated_complexity</th>\n",
       "      <th>Author</th>\n",
       "      <th>hash</th>\n",
       "      <th>center</th>\n",
       "      <th>fixed_cluster</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>13.177872</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>7.175000</td>\n",
       "      <td>7.525000</td>\n",
       "      <td>26.355745</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75</td>\n",
       "      <td>573.979592</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>38.437500</td>\n",
       "      <td>40.312500</td>\n",
       "      <td>1147.959184</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.050000</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.004192</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>266.518875</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>24.087500</td>\n",
       "      <td>25.262500</td>\n",
       "      <td>533.037750</td>\n",
       "      <td>A. Unique TensorFlower</td>\n",
       "      <td>0ef962b1a5b4e80a7029b0b159af6817b12a04df</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>58.992683</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>117.985366</td>\n",
       "      <td>George Karpenkov</td>\n",
       "      <td>ef47bbbd57cba8fcc7ae11df8c7141d6c68ba0d0</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709757</th>\n",
       "      <td>175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.586576</td>\n",
       "      <td>99.679797</td>\n",
       "      <td>156.212240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709758</th>\n",
       "      <td>41</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>31.297426</td>\n",
       "      <td>23.353552</td>\n",
       "      <td>36.598296</td>\n",
       "      <td>4808.025790</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709759</th>\n",
       "      <td>157</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>119.846242</td>\n",
       "      <td>89.427018</td>\n",
       "      <td>140.144696</td>\n",
       "      <td>3068.536785</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709760</th>\n",
       "      <td>574</td>\n",
       "      <td>33866.000000</td>\n",
       "      <td>438.163968</td>\n",
       "      <td>326.949734</td>\n",
       "      <td>512.376148</td>\n",
       "      <td>661904.883795</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.764767</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709761</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.740333</td>\n",
       "      <td>10.252779</td>\n",
       "      <td>16.067545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Ben Alpert</td>\n",
       "      <td>a15603d8f1ecdd673d80be318293cee53eb4475d</td>\n",
       "      <td>0.044030</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>708848 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        total_changed  rated_complexity  total_dmm_unit_complexity  \\\n",
       "0                  14         13.177872                  14.000000   \n",
       "1                  75        573.979592                  75.000000   \n",
       "2                   4          0.301887                   4.000000   \n",
       "3                  47        266.518875                  47.000000   \n",
       "4                  19         58.992683                  18.050000   \n",
       "...               ...               ...                        ...   \n",
       "709757            175          0.000000                 133.586576   \n",
       "709758             41        246.000000                  31.297426   \n",
       "709759            157        157.000000                 119.846242   \n",
       "709760            574      33866.000000                 438.163968   \n",
       "709761             18          0.000000                  13.740333   \n",
       "\n",
       "        total_dmm_size  total_dmm_unit_interfacing  scaled_rated_complexity  \\\n",
       "0             7.175000                    7.525000                26.355745   \n",
       "1            38.437500                   40.312500              1147.959184   \n",
       "2             2.050000                    2.150000                 0.603774   \n",
       "3            24.087500                   25.262500               533.037750   \n",
       "4            18.050000                   18.050000               117.985366   \n",
       "...                ...                         ...                      ...   \n",
       "709757       99.679797                  156.212240                 0.000000   \n",
       "709758       23.353552                   36.598296              4808.025790   \n",
       "709759       89.427018                  140.144696              3068.536785   \n",
       "709760      326.949734                  512.376148            661904.883795   \n",
       "709761       10.252779                   16.067545                 0.000000   \n",
       "\n",
       "                        Author                                      hash  \\\n",
       "0       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "1       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "2       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "3       A. Unique TensorFlower  0ef962b1a5b4e80a7029b0b159af6817b12a04df   \n",
       "4             George Karpenkov  ef47bbbd57cba8fcc7ae11df8c7141d6c68ba0d0   \n",
       "...                        ...                                       ...   \n",
       "709757              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709758              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709759              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709760              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "709761              Ben Alpert  a15603d8f1ecdd673d80be318293cee53eb4475d   \n",
       "\n",
       "          center  fixed_cluster  Cluster  \n",
       "0       0.004192              0        0  \n",
       "1       0.044030              2        3  \n",
       "2       0.004192              0        0  \n",
       "3       0.044030              2        3  \n",
       "4       0.044030              2        3  \n",
       "...          ...            ...      ...  \n",
       "709757  0.172553              3        1  \n",
       "709758  0.044030              2        3  \n",
       "709759  0.172553              3        1  \n",
       "709760  0.764767              4        2  \n",
       "709761  0.044030              2        3  \n",
       "\n",
       "[708848 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the KMeans object based on number discovered above\n",
    "k = 5\n",
    "\n",
    "if (h2o4gpu_enabled == True):\n",
    "    import h2o4gpu\n",
    "    kmeans = h2o4gpu.KMeans(n_gpus=1, n_clusters = k, init='k-means++', max_iter = 20, random_state = 42, backend=h2o4gpu)\n",
    "else:\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "    mix = GaussianMixture(n_components=k,random_state=42)\n",
    "# Learning the Gaussian mixture model from data   \n",
    "mix.fit(data_scaled)\n",
    "# Saving the parameters of Gaussian mixture model in a file\n",
    "import pickle\n",
    "vfilename = 'C:/Users/aveli/Downloads/gmmclust_model.sav'\n",
    "pickle.dump(mix, open(vfilename, 'wb'))\n",
    "# Predicting the cluster labels of the data for the Gaussian mixture model\n",
    "cluster_frame = pd.DataFrame(data_scaled)\n",
    "gmmhash_clusters = mix.predict(cluster_frame)\n",
    "# Collecting the mean of the Gaussian mixture model in 'gmmcentroids'\n",
    "gmmcentroids = mix.means_\n",
    "\n",
    "combinedCentroids = gmmcentroids[gmmhash_clusters].sum(axis=1)\n",
    "# adding column with combined centroid values to the original dataframe \n",
    "ml_commits_nout['center'] = combinedCentroids\n",
    "#print(combinedCentroids)\n",
    "# Creating a dictionary with combined centroid values and target cluster labels\n",
    "unique_centroids = np.unique(combinedCentroids).tolist()\n",
    "cluster_labels = np.arange(5).tolist()\n",
    "cluster_dict = dict(zip(unique_centroids,cluster_labels))\n",
    "#print(g)\n",
    "ml_commits_nout['fixed_cluster'] = ml_commits_nout['center'].map(cluster_dict)\n",
    "# Converting the input data series into pan\n",
    "ml_commits_nout['Cluster'] = gmmhash_clusters\n",
    "#ml_commits_nout[ml_commits_nout['Cluster']==0]\n",
    "#ml_commits_nout['sum_value'] = ml_commits_nout['scaled_rated_complexity']+ml_commits_nout['total_dmm_unit_interfacing']+ ml_commits_nout['total_dmm_size']+ml_commits_nout['total_dmm_unit_complexity']+ml_commits_nout['rated_complexity']+ml_commits_nout['ratio_changed']+ml_commits_nout['total_changed']\n",
    "#ml_commits_nout[ml_commits_nout['Cluster']==3]  \n",
    "ml_commits_nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values of inverted scaling of centroids for sanity\n",
    "real_centroids = scaler.inverse_transform(gmmcentroids)\n",
    "\n",
    "# Write these to dataframe\n",
    "real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity'])\n",
    "\n",
    "# Add a cloumn for summing all coloumns\n",
    "real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "\n",
    "#You can write it out as csv if required.\n",
    "real_centroids_dataFrame.to_csv('C:/Users/aveli/Downloads/totalCommits_centroids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_commits_nout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.998419971559488\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "from xgboost import XGBClassifier\n",
    "if (h2o4gpu_enabled == True):\n",
    "    import h2o4gpu as sklearn\n",
    "else:\n",
    "    import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ml_commits_nout_numeric_xg = ml_commits_nout.drop(columns=['Author','hash','center','fixed_cluster'])\n",
    "\n",
    "\n",
    "# Prepare the 'X' and 'Y' for the model\n",
    "X_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg.drop(columns = ['Cluster'])\n",
    "Y_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg['Cluster']\n",
    "\n",
    "# Split the data for 'Training' and 'Testing' datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ml_commits_nout_numeric_xg, Y_ml_commits_nout_numeric_xg, random_state=7)\n",
    "\n",
    "xgboostmodel = XGBClassifier()\n",
    "# Training the xgboost classifier\n",
    "xgboostmodel.fit(X_train, y_train)\n",
    "# Predicting the class labels of test data for xgboost classifier\n",
    "y_pred = xgboostmodel.predict(X_test)\n",
    " \n",
    "# accuracy on X_test \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.554363566824674"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if (h2o4gpu_enabled == True):\n",
    "    from h2o4gpu import metrics\n",
    "else:\n",
    "    from sklearn import metrics\n",
    "\n",
    "metrics.silhouette_score(data_scaled,gmmhash_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Saving the parameters of the xgboost classifier in a file\n",
    "filename = 'C:/Users/aveli/Downloads/finalized_model.sav'\n",
    "pickle.dump(xgboostmodel, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
